{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85fe081b",
   "metadata": {},
   "source": [
    "# Cat Emotion Classification Project Report\n",
    "\n",
    "**Supervisor:** [Supervisor Name]\n",
    "**Author:** [Your Name]\n",
    "**Date:** [Current Date]\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Project Overview and Objective\n",
    "\n",
    "This project aims to automatically classify the emotional state of domestic cats from images using Deep Learning.\n",
    "\n",
    "The core goal is to leverage **Transfer Learning** with a pre-trained **ResNet50** model, fine-tuning it to recognize subtle feline facial and body cues across **9 distinct emotion classes**:\n",
    "`'angry', 'disgusted', 'happy', 'normal', 'relaxed', 'sad', 'scared', 'surprised', 'uncomfortable'`.\n",
    "\n",
    "### 1.1. Project Structure and Tools\n",
    "\n",
    "The project adheres to a standard, maintainable Machine Learning (ML) structure:\n",
    "\n",
    "| Component | Purpose | Technology |\n",
    "| :--- | :--- | :--- |\n",
    "| `main.py` | Manages the full training/evaluation pipeline. | PyTorch, MLflow |\n",
    "| `src/preprocess.py` | Contains all data loading and transformation logic. | PyTorch, torchvision |\n",
    "| `data/` | Stores the segregated image dataset (`train`, `val`, `test`). | DVC-tracked |\n",
    "| **MLflow** | Experiment tracking, parameter/metric logging, and best model versioning. | MLflow |\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Data Exploration and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b94d04f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'run_eda_and_print_config' from 'src.preprocess' (d:\\Abdalrhman\\Cat-Emotion-Detector\\src\\preprocess.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 14\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmlflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Import the core logic from the modular code for consistency\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# NOTE: Ensure src/preprocess.py is correctly updated with run_eda_and_print_config\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocess\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m run_eda_and_print_config, train_transforms\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'run_eda_and_print_config' from 'src.preprocess' (d:\\Abdalrhman\\Cat-Emotion-Detector\\src\\preprocess.py)"
     ]
    }
   ],
   "source": [
    "# 2.1. Setup and Module Imports\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torchvision import datasets, transforms\n",
    "from PIL import Image\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "\n",
    "# Import the core logic from the modular code for consistency\n",
    "# NOTE: Ensure src/preprocess.py is correctly updated with run_eda_and_print_config\n",
    "from src.preprocess import run_eda_and_print_config, train_transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92764e52",
   "metadata": {},
   "source": [
    "### 2.2. Data Summary and Class Imbalance Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a715a2c1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'run_eda_and_print_config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m DATA_DIR \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mAbdalrhman\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mCat-Emotion-Detector\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mfinal_data\u001b[39m\u001b[38;5;124m\"\u001b[39m \n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Run the EDA and print the configuration details (executed once)\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[43mrun_eda_and_print_config\u001b[49m(DATA_DIR) \n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Conclusion: The wide variance in class weights confirms significant class imbalance. \u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# This is addressed by passing the computed class weights to PyTorch's CrossEntropyLoss function in main.py.\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'run_eda_and_print_config' is not defined"
     ]
    }
   ],
   "source": [
    "# The DATA_DIR should match the path used in main.py and src/preprocess.py\n",
    "DATA_DIR = r\"D:\\Abdalrhman\\Cat-Emotion-Detector\\data\\final_data\" \n",
    "\n",
    "# Run the EDA and print the configuration details (executed once)\n",
    "run_eda_and_print_config(DATA_DIR) \n",
    "\n",
    "# Conclusion: The wide variance in class weights confirms significant class imbalance. \n",
    "# This is addressed by passing the computed class weights to PyTorch's CrossEntropyLoss function in main.py."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73f54ed",
   "metadata": {},
   "source": [
    "### 2.3. Data Preprocessing and Augmentation Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52c9406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants used for denormalization visualization\n",
    "IMAGENET_MEAN = np.array([0.485, 0.456, 0.406])\n",
    "IMAGENET_STD = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "def denormalize_and_show(tensor, mean, std):\n",
    "    \"\"\"Helper to convert tensor back to plottable image after reversing normalization.\"\"\"\n",
    "    img_to_show = tensor.numpy().transpose((1, 2, 0)) # C, H, W -> H, W, C\n",
    "    img_to_show = std * img_to_show + mean\n",
    "    img_to_show = np.clip(img_to_show, 0, 1)\n",
    "    return img_to_show\n",
    "\n",
    "# Load one sample image for visualization purposes\n",
    "base_train_dataset = datasets.ImageFolder(os.path.join(DATA_DIR, \"train\"))\n",
    "img_path, label_idx = base_train_dataset.imgs[10] # Selecting an arbitrary image\n",
    "original_img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "plt.figure(figsize=(18, 5))\n",
    "\n",
    "# Plot Original\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.imshow(original_img)\n",
    "plt.title(f\"1. Original Image\\nLabel: {base_train_dataset.classes[label_idx]}\")\n",
    "plt.axis('off')\n",
    "\n",
    "# Plot 3 Augmented Examples\n",
    "for i in range(3):\n",
    "    # Apply the defined training transforms\n",
    "    augmented_tensor = train_transforms(original_img) \n",
    "    augmented_img = denormalize_and_show(augmented_tensor, IMAGENET_MEAN, IMAGENET_STD)\n",
    "    \n",
    "    plt.subplot(1, 4, i + 2)\n",
    "    plt.imshow(augmented_img)\n",
    "    plt.title(f\"{i+2}. Augmentation (Crop, Flip, Color Jitter)\")\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.suptitle(\"Visualization of Data Augmentation Techniques on Training Data\", fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "print(\"Summary: RandomResizedCrop, RandomFlip, ColorJitter, and Normalization are applied to the training set to increase model robustness.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
